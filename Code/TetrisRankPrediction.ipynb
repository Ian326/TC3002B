{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict in-game Rank Tetr.io\n",
    "---\n",
    "**Por: Ian Joab Padron Corona - A01708940**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ian326/TC3002B/blob/main/Code/TetrisRankPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "===============================================================================\n",
    "Librerias de Python a utilizar en el proyecto\n",
    "===============================================================================\n",
    "pandas: Libreria de manipulacion de datos en DataFrames\n",
    "numpy: Libreria para operaciones matematicas y manipulacion de arreglos\n",
    "seaborn: Libreria para visualizacion de datos\n",
    "matplotlib: Libreria para graficar\n",
    "tensorflow: Libreria de aprendizaje profundo\n",
    "keras: API de alto nivel para crear redes neuronales\n",
    "Sequential: API para crear modelos de redes neuronales\n",
    "Dense: Capa densa (fully connected)\n",
    "OneHotEncoder: Convertir variables categoricas en numericas creando columnas binarias. ESTE SOLO PARA EL DE TETRIS\n",
    "MinMaxScaler: Normaliza los datos entre 0 y 1\n",
    "train_test_split: Divide los datos en conjuntos de entrenamiento y prueba\n",
    "classification_report: Genera un informe de clasificacion\n",
    "confusion_matrix: Crea una matriz de confusiones\n",
    "===============================================================================\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../content/tl-data-09-2023.csv', low_memory=False)\n",
    "# Eliminar columnas innecesarias\n",
    "data.drop(columns=['id', 'username', 'country', 'bestrank', '40l_sprint', 'blitz'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot(figsize = (16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Escalamiento de las columnas con valores muy grandes\n",
    "data_scaled = data.copy()\n",
    "rank_column = data_scaled['rank']\n",
    "data_scaled.drop(columns=['rank'], inplace=True)\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data_scaled), columns=data_scaled.columns)\n",
    "data_scaled['rank'] = rank_column\n",
    "\n",
    "del rank_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.boxplot(figsize = (16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Apply OneHotEncoder to the 'rank' column\n",
    "rank_encoded = encoder.fit_transform(data[['rank']])\n",
    "\n",
    "# Convert the encoded result to a DataFrame\n",
    "rank_encoded_df = pd.DataFrame(rank_encoded, columns=encoder.get_feature_names_out(['rank']))\n",
    "\n",
    "rank_columns = rank_encoded_df.columns.tolist()\n",
    "\n",
    "# Concatenate the encoded 'rank' column back to the original DataFrame\n",
    "data_scaled = pd.concat([data_scaled, rank_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'rank' column\n",
    "data_scaled = data_scaled.drop(columns=['rank'])\n",
    "\n",
    "del rank_encoded_df, rank_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar las 'features; X' y la 'target variable; y'\n",
    "X = data_scaled.drop(columns=rank_columns)\n",
    "y = data_scaled[rank_columns]\n",
    "\n",
    "# Dividir los datos en en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                    X,\n",
    "                                                    y,\n",
    "                                                    train_size   = 0.80,\n",
    "                                                    random_state = 42,\n",
    "                                                    shuffle      = True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simple(shape):\n",
    "    \"\"\"\n",
    "    This is a simple model multilayer perceptron or neural network.\n",
    "    Your function should return the model.\n",
    "    \"\"\"\n",
    "    model  =  Sequential([\n",
    "                    Dense(96, activation='relu', input_shape=shape),\n",
    "                    Dense(96, activation='relu'),\n",
    "                    Dense(17, activation='softmax')\n",
    "            ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_simple(model):\n",
    "    \"\"\"\n",
    "    This function takes in the model returned from your get_model function, and compiles it with an optimiser,\n",
    "    loss function and metric.\n",
    "    Compile the model using the Adam optimiser (with default settings), the cross-entropy loss function and\n",
    "    accuracy as the only metric.\n",
    "    Your function doesn't need to return anything; the model will be compiled in-place.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer= \"adam\",\n",
    "                  loss = \"categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_features, train_labels):\n",
    "    \"\"\"\n",
    "    Train the model on the scaled_train_images and train_labels.\n",
    "    Your function should return the training history, as returned by model.fit.\n",
    "    \"\"\"\n",
    "    return model.fit(\n",
    "                        x=train_features, \n",
    "                        y=train_labels, \n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = model_simple(X_train.shape[1:])\n",
    "model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model_simple(model_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_nn = train_model_simple(model_nn, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_trainingAcc = pd.DataFrame(history_nn.history['accuracy'])\n",
    "frame_testingAcc = pd.DataFrame(history_nn.history['val_accuracy'])\n",
    "frame_trainingLoss = pd.DataFrame(history_nn.history['loss'])\n",
    "frame_testingLoss = pd.DataFrame(history_nn.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(frame_trainingAcc, label='Training')\n",
    "plt.plot(frame_testingAcc, label='Testing')\n",
    "plt.title('Accuracy vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(frame_trainingLoss, label='Training')\n",
    "plt.plot(frame_testingLoss, label='Testing')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    This function should evaluate the model on the scaled_test_images and test_labels.\n",
    "    Your function should return a tuple (test_loss, test_accuracy).\n",
    "    \"\"\"\n",
    "    test_loss, test_accuracy = model.evaluate(test_features, test_labels)\n",
    "    return (test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model_nn, X_test, y_test)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones y métricas\n",
    "preds = model_nn.predict(X_test)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "y_true = np.argmax(y_test.values, axis=1)\n",
    "class_labels = list(y_test.columns)\n",
    "\n",
    "print(\"\\n Reporte de clasificación:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
